{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NE extraction: low computational cost but runs for hours (2h-3h?)\n",
    "- NER (with Spacy / StanfordNLP) based on all evidence\n",
    "- processed data is ./data/spacy_ner_tot.json\n",
    "    - It is a match dictionary from NE to evidence\n",
    "    - e.g. \"New York City\": [\"evidence-0000\", \"evidence-00000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 21:06:38.246810: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-14 21:06:38.462509: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-14 21:06:39.491627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-05-14 21:06:40.554926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-14 21:06:40.581444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-14 21:06:40.581804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import spacy\n",
    "import os\n",
    "import string\n",
    "\n",
    "EVI_PATH = \"./data/evidence.json\"\n",
    "PATH_TO_JAR = \"../../software/stanford_ner/stanford-ner.jar\"\n",
    "PATH_TO_MODEL = \"../../software/stanford_ner/classifiers/english.all.3class.distsim.crf.ser.gz\"\n",
    "FILE_PRE = \"./data/spacy_ner/ner_match_\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ner_tagger = StanfordNERTagger(model_filename=PATH_TO_MODEL,path_to_jar=PATH_TO_JAR, encoding='utf-8')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "puncu_remove = str.maketrans('', '', string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi_df = pd.read_json(EVI_PATH, orient=\"index\").reset_index().rename({0: \"evidence\", \"index\": \"evi_id\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: ./data/spacy_ner/ner_match_4.json, epoch: 0\n",
      "File saved: ./data/spacy_ner/ner_match_4.json, epoch: 50\n",
      "File saved: ./data/spacy_ner/ner_match_4.json, epoch: 100\n"
     ]
    }
   ],
   "source": [
    "ner_map = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for id, evidence in evi_df.values[:1000]:\n",
    "    doc = nlp(evidence)\n",
    "    for ent in doc.ents:\n",
    "        ner_map[ent.label_][ent.text.lower()].append(id)\n",
    "\n",
    "\n",
    "# total_epo: 1208.827\n",
    "START_EPO = 0\n",
    "END_EPO = 1210\n",
    "FILE_NO = 4\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# Loop through evidence\n",
    "ner_map = defaultdict(lambda: defaultdict(list))\n",
    "for epoch in range(START_EPO, END_EPO):\n",
    "    cur_data = evi_df.loc[epoch*BATCH_SIZE:(epoch+1)*BATCH_SIZE].values\n",
    "    \n",
    "    ## NE extraction\n",
    "    for id, evidence in cur_data:\n",
    "        doc = nlp(evidence)\n",
    "        for ent in doc.ents:\n",
    "            ner_map[ent.label_][ent.text.lower().translate(puncu_remove)].append(id)\n",
    "    \n",
    "    ## save file\n",
    "    if epoch%50 == 0:\n",
    "        file_name = FILE_PRE + str(FILE_NO) + \".json\"\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(ner_map, f)\n",
    "        print(f\"File saved: {file_name}, epoch: {epoch}\")\n",
    "    \n",
    "## save work\n",
    "file_name = FILE_PRE + str(FILE_NO) + \".json\"\n",
    "with open(file_name, 'w') as f:\n",
    "    json.dump(ner_map, f)\n",
    "print(f\"File saved: {file_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine saved file\n",
    "\n",
    "parent_dir = \"./data/spacy_ner/\"\n",
    "\n",
    "\n",
    "file_names = os.listdir(parent_dir)\n",
    "\n",
    "\n",
    "print(file_names)\n",
    "\n",
    "total_dict = defaultdict(lambda: defaultdict(list))\n",
    "for file_name in file_names[0:]:\n",
    "    with open(parent_dir + file_name, \"r\") as cur_file:\n",
    "        cur_dict = json.loads(cur_file.read())\n",
    "    for ner_key in cur_dict.keys():\n",
    "        for key in cur_dict[ner_key].keys():\n",
    "            total_dict[ner_key][key]+=cur_dict[ner_key][key]\n",
    "\n",
    "file_name = f\"./data/spacy_ner_tot.json\"\n",
    "with open(file_name, 'w') as f:\n",
    "    json.dump(ner_map, f)\n",
    "print(f\"File saved: {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
