{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claim verification Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
    "# from transformers import RobertaForSequenceClassification, RobertaTokenizerFast, BertConfig\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "TRAIN_PATH = \"./data/train-claims.json\"\n",
    "EVI_PATH = \"./data/evidence.json\"\n",
    "PAD = \"[PAD]\"\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "PAD_LEN = 50\n",
    "RANDOM_SEED = 42\n",
    "SUPPORTS = \"SUPPORTS\"\n",
    "REFUTES = \"REFUTES\"\n",
    "NOT_ENOUGH_INFO = \"NOT_ENOUGH_INFO\"\n",
    "DISPUTED = \"DISPUTED\"\n",
    "RELATED = 1\n",
    "MAX_EVI = 3\n",
    "NOT_RELATED = 0\n",
    "num_class = 4\n",
    "label_trans = {SUPPORTS: 0, REFUTES: 1, NOT_ENOUGH_INFO: 2, DISPUTED: 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                              claim     label  \\\n",
      "0      0  Not only is there no scientific evidence that ...  DISPUTED   \n",
      "\n",
      "                                            evidence  \n",
      "0  [At very high concentrations (100 times atmosp...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def relation_sep(claim, evidence, max_evi):\n",
    "    ## Match the input into Bert input formate, including adding [CLS] and [SEP]\n",
    "    ## inputs are tokens\n",
    "    output = [CLS] + claim + [SEP]\n",
    "    seg_li = [0 for _ in range(len(claim)+2)]\n",
    "    cur_seg = 1\n",
    "    for evi in evidence:\n",
    "        output += evi + [SEP]\n",
    "        seg_li += [cur_seg for _ in range(len(evi)+1)]\n",
    "        # cur_seg += 1\n",
    "    for i in range(len(evidence), max_evi):\n",
    "        output += [PAD for _ in range(len(evi))] + [SEP]\n",
    "        seg_li += [cur_seg for _ in range(len(evi)+1)]\n",
    "        # cur_seg += 1\n",
    "    return output, seg_li\n",
    "\n",
    "## read file\n",
    "claim_df = pd.read_json(TRAIN_PATH, orient=\"index\")\n",
    "evi_df = pd.read_json(EVI_PATH, orient=\"index\").rename({0: \"evi_text\"}, axis=1)\n",
    "veri_df = claim_df.rename({\"claim_text\": \"claim\", \"claim_label\": \"label\", \"evidences\": \"evidence\"}, axis=1).reset_index(drop=True)\n",
    "veri_df[\"evidence\"] = veri_df.apply(lambda x: [evi_df.loc[i].values[0] for i in x[\"evidence\"]], axis=1)\n",
    "veri_df[\"evidence\"] = veri_df.apply(lambda x: x.evidence if len(x.evidence) <= MAX_EVI else x.evidence[:MAX_EVI], axis=1)\n",
    "\n",
    "## Resample with different order of evidence\n",
    "sample_pos = 0.4\n",
    "new_sample_df = pd.DataFrame(columns = [\"claim\", \"label\", \"evidence\"])\n",
    "for cur_record in veri_df.values:\n",
    "    if random.random() < sample_pos and len(cur_record[2])>1:\n",
    "        new_sample = random.sample(cur_record[2], len(cur_record[2]))\n",
    "        while(new_sample != cur_record[2]):\n",
    "            new_sample = random.sample(cur_record[2], len(cur_record[2]))\n",
    "        new_sample_df = new_sample_df.append({\"claim\": cur_record[0], \"evidence\": new_sample, \"label\": cur_record[1]}, ignore_index=True)\n",
    "\n",
    "\n",
    "veri_df = pd.concat([veri_df, new_sample_df]).reset_index()\n",
    "print(veri_df.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_sen(input, max_len):\n",
    "    ## Add padding to sequence\n",
    "    if len(input) > max_len: return input[:max_len]\n",
    "    return input+[PAD for _ in range(max_len-len(input))]\n",
    "\n",
    "def saperate_evi(claim, evidence):\n",
    "    ## seperate and concatenate claim and evidence\n",
    "    output = [CLS] + claim + [SEP] + evidence + [SEP]\n",
    "    seg_li = [0 for _ in range(len(claim)+2)] + [1 for _ in range(len(evidence)+1)]\n",
    "    return output, seg_li\n",
    "\n",
    "class RelClassifier(nn.Module):\n",
    "    def __init__(self, num_class, dropout = 0.1):\n",
    "        super(RelClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertForSequenceClassification.from_pretrained('bert-base-uncased', problem_type=\"multi_label_classification\")\n",
    "        \n",
    "        #Classification layer\n",
    "        #input dimension is 768 because [CLS] embedding has a dimension of 768\n",
    "        self.linear = nn.Linear(768, num_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg_li):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "        \n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, token_type_ids = seg_li, output_hidden_states=True)\n",
    "        cont_reps = outputs.hidden_states[-1][:,0]\n",
    "        dropout_output = self.dropout(cont_reps)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        return linear_output\n",
    "    \n",
    "def get_accuracy(output, labels):\n",
    "    # return accuracy of the prediction based on class score\n",
    "    return (output.argmax(dim=1) == labels.argmax(dim=1)).sum().item() / len(labels)\n",
    "\n",
    "def evaluate(b_model, criterion, dataloader, gpu):\n",
    "    ## evaluate accuracy and loss\n",
    "    b_model.eval()\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, seg_li, labels in dataloader:\n",
    "            seq, attn_masks, seg_li, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_li.cuda(gpu), labels.cuda(gpu)\n",
    "            output = b_model(seq, attn_masks, seg_li).cuda(gpu)\n",
    "            mean_loss += criterion(output, labels)\n",
    "            mean_acc += get_accuracy(output, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count\n",
    "\n",
    "def train(b_model, criterion, opti, train_loader, dev_loader, max_eps, gpu, file_pre = \"model_\"):\n",
    "    ## Train model\n",
    "    best_acc = 0\n",
    "    best_loss = 99\n",
    "    st = time.time()\n",
    "    for ep in range(max_eps):\n",
    "        \n",
    "        b_model.train()\n",
    "        for it, (seq, attn_masks, seg_li, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, seg_li, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_li.cuda(gpu), labels.cuda(gpu)\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            output = b_model(seq, attn_masks, seg_li)\n",
    "            \n",
    "            #Computing loss\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "            \n",
    "            ## print\n",
    "            if it % 30 == 0:\n",
    "                acc = get_accuracy(output, labels)\n",
    "                print(\"O: \", output[:3])\n",
    "                print(\"L: \", labels[:3])\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "        \n",
    "        ## save best model\n",
    "        dev_acc, dev_loss = evaluate(b_model, criterion, dev_loader, gpu)\n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
    "        if dev_acc > best_acc and dev_loss <= best_loss:\n",
    "            print(\"Accuracy improved from {} to {}, Loss improved from {} to {}, saving model...\".format(best_acc, dev_acc, best_loss, dev_loss))\n",
    "            print(\"Saved: \" + file_pre + 'model.dat')\n",
    "            best_acc = dev_acc\n",
    "            best_loss = dev_loss\n",
    "            torch.save(b_model.state_dict(), file_pre + 'model.dat')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(Dataset):\n",
    "    def __init__(self, input_df, max_len_claim, max_len_evi, max_evi, num_class):\n",
    "        self.df = input_df\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_len_claim = max_len_claim\n",
    "        self.max_len_evi = max_len_evi\n",
    "        self.max_evi = max_evi\n",
    "        self.num_class = num_class\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ## claim_text, claim_label, evidences\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        claim = self.df.loc[index, 'claim']\n",
    "        label = self.df.loc[index, 'label']\n",
    "        evidence = self.df.loc[index, 'evidence']\n",
    "\n",
    "        # Preprocessing the claim, tokenization, padding, getting attention tag, getting category tag, put to GPU\n",
    "        claim = self.tokenizer.tokenize(claim)\n",
    "        evidence = [self.tokenizer.tokenize(i) for i in evidence]\n",
    "        claim = pad_sen(claim, self.max_len_claim)\n",
    "        evidence = [pad_sen(i, self.max_len_evi) for i in evidence]\n",
    "        input_token, seg_li = relation_sep(claim, evidence, self.max_evi)\n",
    "        attn_mask = [1 if token != PAD else 0 for token in input_token]\n",
    "        input_token = self.tokenizer.convert_tokens_to_ids(input_token)\n",
    "        input_id = torch.tensor(input_token) #Converting the list to a pytorch tensor\n",
    "        attn_mask = torch.tensor(attn_mask)\n",
    "        seg_li = torch.tensor(seg_li)\n",
    "        labels = np.zeros(self.num_class)\n",
    "        np.put(labels, label, 1)\n",
    "        label = torch.tensor(labels)\n",
    "\n",
    "        \n",
    "        return input_id, attn_mask, seg_li, label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FRA = 0.2\n",
    "\n",
    "# read data\n",
    "train_veri_df, test_veri_df = train_test_split(veri_df, test_size = TEST_FRA, shuffle=True, random_state=RANDOM_SEED)\n",
    "train_veri_df[\"label\"] = train_veri_df[\"label\"].apply(lambda x: label_trans[x])\n",
    "test_veri_df[\"label\"] = test_veri_df[\"label\"].apply(lambda x: label_trans[x])\n",
    "train_veri_df = train_veri_df.reset_index(drop=True)\n",
    "test_veri_df = test_veri_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## read trained data\n",
    "PAD_CLAIM = 64\n",
    "PAD_EVIDENCE = 64\n",
    "BATCH_SIZE = 10\n",
    "NO_WORKER = 4\n",
    "MAX_EVI = 3\n",
    "num_class_val = 4\n",
    "\n",
    "\n",
    "# prepare data loader\n",
    "train_veri_ds = ValDataset(train_veri_df, PAD_CLAIM, PAD_EVIDENCE, MAX_EVI, num_class)\n",
    "train_veri_dl = DataLoader(train_veri_ds, batch_size = BATCH_SIZE, num_workers = NO_WORKER)\n",
    "\n",
    "test_veri_ds = ValDataset(test_veri_df, PAD_CLAIM, PAD_EVIDENCE, MAX_EVI, num_class)\n",
    "test_veri_dl = DataLoader(test_veri_ds, batch_size = BATCH_SIZE, num_workers = NO_WORKER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O:  tensor([[-0.6654,  0.2820, -0.3985,  0.7442],\n",
      "        [ 0.2701,  0.2534, -0.1323,  0.0957],\n",
      "        [ 0.2245, -0.3873, -0.1946,  0.6253]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 0 of epoch 0 complete. Loss: 1.2129192155815567; Accuracy: 0.3; Time taken (s): 2.466339111328125\n",
      "O:  tensor([[-0.0678, -0.4956, -0.2541,  0.4276],\n",
      "        [ 0.9738, -0.4358,  0.0339, -0.2770],\n",
      "        [ 0.2607, -0.4874, -0.0370,  0.2732]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 30 of epoch 0 complete. Loss: 1.5902034730444428; Accuracy: 0.4; Time taken (s): 11.029784679412842\n",
      "O:  tensor([[ 0.9879, -1.5796,  0.1103, -0.1894],\n",
      "        [ 0.5925, -0.6061,  0.2398, -0.0989],\n",
      "        [-0.9534, -0.3553,  1.0800, -0.7018]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 60 of epoch 0 complete. Loss: 1.2828058505228042; Accuracy: 0.4; Time taken (s): 10.019209146499634\n",
      "O:  tensor([[ 0.1095,  0.3834, -0.3454, -0.0027],\n",
      "        [ 0.2333,  0.2699,  0.1862, -0.4216],\n",
      "        [ 0.4952, -0.1475, -0.2685,  0.6090]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 90 of epoch 0 complete. Loss: 1.5132345284595823; Accuracy: 0.4; Time taken (s): 9.857675313949585\n",
      "O:  tensor([[ 0.5083, -1.2978,  1.7195, -0.2276],\n",
      "        [ 0.0529, -0.2613, -0.3185, -0.1470],\n",
      "        [-0.3583, -0.3785,  0.1664, -0.5106]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 120 of epoch 0 complete. Loss: 0.9839910633224549; Accuracy: 0.6; Time taken (s): 9.881621599197388\n",
      "Epoch 0 complete! Development Accuracy: 0.4058823529411765; Development Loss: 1.1981179464082024\n",
      "Accuracy improved from 0 to 0.4058823529411765, Loss improved from 99 to 1.1981179464082024, saving model...\n",
      "Saved: val_model.dat\n",
      "O:  tensor([[-0.5532,  0.9502, -0.6970, -0.3875],\n",
      "        [ 0.7065,  0.3487,  0.1886, -1.0255],\n",
      "        [ 0.1675,  0.3949, -0.3708, -0.3054]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 0 of epoch 1 complete. Loss: 1.042136033527698; Accuracy: 0.6; Time taken (s): 9.66063666343689\n",
      "O:  tensor([[ 0.1678, -0.2045,  0.1202,  0.9850],\n",
      "        [ 0.5605, -0.1058,  0.4842, -0.4265],\n",
      "        [-0.5427, -0.1428,  1.4455,  0.0940]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 30 of epoch 1 complete. Loss: 1.8125282602378754; Accuracy: 0.4; Time taken (s): 11.171839237213135\n",
      "O:  tensor([[ 1.0122, -1.7908,  1.2981,  0.4993],\n",
      "        [ 0.4928, -0.4879, -0.0899,  0.2729],\n",
      "        [-0.1872, -1.5966,  1.4740, -1.0117]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 60 of epoch 1 complete. Loss: 1.2087532023195275; Accuracy: 0.6; Time taken (s): 10.008457660675049\n",
      "O:  tensor([[ 0.9680,  0.3452, -1.7602, -0.1889],\n",
      "        [-1.0558, -0.3382,  0.3666, -0.8159],\n",
      "        [ 1.6780, -0.7788,  0.4718, -0.4267]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 90 of epoch 1 complete. Loss: 1.1394866105992378; Accuracy: 0.6; Time taken (s): 9.979499101638794\n",
      "O:  tensor([[-0.5622, -1.2963,  2.0954, -0.9922],\n",
      "        [-0.0265, -1.1839,  0.2432,  1.1974],\n",
      "        [ 0.1932,  1.4169, -2.0803,  0.2787]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 120 of epoch 1 complete. Loss: 0.6049136622993241; Accuracy: 0.8; Time taken (s): 9.992623090744019\n",
      "Epoch 1 complete! Development Accuracy: 0.6573529411764704; Development Loss: 0.9165018446920885\n",
      "Accuracy improved from 0.4058823529411765 to 0.6573529411764704, Loss improved from 1.1981179464082024 to 0.9165018446920885, saving model...\n",
      "Saved: val_model.dat\n",
      "O:  tensor([[-0.4617,  1.0060, -1.7550,  0.1010],\n",
      "        [ 0.4756,  0.2606,  0.0320, -0.8013],\n",
      "        [ 1.5302,  0.2238, -0.6725, -1.6887]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 0 of epoch 2 complete. Loss: 0.7114327754481993; Accuracy: 0.7; Time taken (s): 9.270231008529663\n",
      "O:  tensor([[-0.4099, -0.6949, -1.7504,  1.5850],\n",
      "        [-0.3203,  0.7790, -1.4706, -0.0651],\n",
      "        [-1.1992, -1.2499,  3.1536, -1.8411]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 30 of epoch 2 complete. Loss: 0.9756890793073597; Accuracy: 0.7; Time taken (s): 10.60530138015747\n",
      "O:  tensor([[ 0.9694, -2.3762, -0.5485,  2.2315],\n",
      "        [ 0.9519, -1.4965, -1.4614,  2.6739],\n",
      "        [-0.5000, -1.0572,  2.7714, -0.8214]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 60 of epoch 2 complete. Loss: 0.615456450161815; Accuracy: 0.7; Time taken (s): 10.044433832168579\n",
      "O:  tensor([[ 0.3374,  2.0458, -2.1400,  0.4637],\n",
      "        [-0.7191,  1.8625, -0.1609, -1.3666],\n",
      "        [ 1.6057, -0.3468, -0.0600, -0.5841]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 90 of epoch 2 complete. Loss: 0.3844071439236122; Accuracy: 0.9; Time taken (s): 10.030307292938232\n",
      "O:  tensor([[ 0.2241, -1.9890,  3.2593, -1.2518],\n",
      "        [ 1.2563, -1.1910, -0.5461,  1.3174],\n",
      "        [ 1.3780,  1.7183, -1.8459, -1.3323]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 120 of epoch 2 complete. Loss: 0.2738117456878877; Accuracy: 0.9; Time taken (s): 10.03691554069519\n",
      "Epoch 2 complete! Development Accuracy: 0.7132352941176471; Development Loss: 0.849414993858312\n",
      "Accuracy improved from 0.6573529411764704 to 0.7132352941176471, Loss improved from 0.9165018446920885 to 0.849414993858312, saving model...\n",
      "Saved: val_model.dat\n",
      "O:  tensor([[-0.0152,  2.5406, -2.2131, -0.8156],\n",
      "        [ 1.9208, -2.0120,  1.2806, -1.3655],\n",
      "        [ 2.6631,  1.3337, -1.8784, -1.8501]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 0 of epoch 3 complete. Loss: 0.4815889360439286; Accuracy: 0.8; Time taken (s): 9.172951221466064\n",
      "O:  tensor([[-0.2859, -1.6808, -1.5362,  1.6632],\n",
      "        [ 0.2284, -1.3862, -1.7795,  2.8928],\n",
      "        [-1.5959, -1.0975,  3.3583, -1.5462]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 30 of epoch 3 complete. Loss: 0.4241734603381976; Accuracy: 0.9; Time taken (s): 10.566075563430786\n",
      "O:  tensor([[ 1.1164, -1.1287, -0.7519,  2.7520],\n",
      "        [ 1.4634, -1.9540, -1.8263,  1.5071],\n",
      "        [-1.8268, -0.8805,  3.1553, -2.5291]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 60 of epoch 3 complete. Loss: 0.22626283149558885; Accuracy: 0.8; Time taken (s): 10.052737951278687\n",
      "O:  tensor([[-0.1290,  3.8575, -1.5442, -1.8597],\n",
      "        [-2.6437,  3.4318,  0.7814, -2.3013],\n",
      "        [ 2.4793, -2.2643,  0.5070, -1.1707]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 90 of epoch 3 complete. Loss: 0.2617216102115846; Accuracy: 1.0; Time taken (s): 10.056183815002441\n",
      "O:  tensor([[ 0.6229, -1.9541,  4.2009, -3.1786],\n",
      "        [ 2.5228, -2.5963,  1.5773, -0.7836],\n",
      "        [ 0.3079,  2.9484, -2.0715, -1.1615]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 120 of epoch 3 complete. Loss: 0.08072873970139667; Accuracy: 1.0; Time taken (s): 10.063935279846191\n",
      "Epoch 3 complete! Development Accuracy: 0.7132352941176471; Development Loss: 0.917279486833283\n",
      "O:  tensor([[-0.2157,  3.1721, -2.3363, -1.3118],\n",
      "        [ 1.9350, -1.2416, -2.0810,  0.6425],\n",
      "        [ 2.3327,  2.2678, -1.2906, -2.5516]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 0 of epoch 4 complete. Loss: 0.3062998868268893; Accuracy: 0.8; Time taken (s): 8.765685319900513\n",
      "O:  tensor([[ 1.2564, -1.9673, -2.4023,  2.7592],\n",
      "        [ 1.0319, -1.7143, -2.3720,  2.9139],\n",
      "        [-0.6796, -1.0443,  4.1602, -2.5455]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 30 of epoch 4 complete. Loss: 0.13260799617523494; Accuracy: 1.0; Time taken (s): 10.409753561019897\n",
      "O:  tensor([[ 0.6525, -2.4116, -2.7549,  4.0491],\n",
      "        [ 2.2360, -1.4485, -3.3537,  2.0255],\n",
      "        [-0.2699, -1.2618,  3.7415, -2.5909]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 60 of epoch 4 complete. Loss: 0.10723268672258045; Accuracy: 1.0; Time taken (s): 10.064090728759766\n",
      "O:  tensor([[ 2.4265,  0.7167, -2.4849,  0.8541],\n",
      "        [-1.5156,  3.9582, -1.6399, -1.2834],\n",
      "        [ 4.1135, -2.5626, -0.5187, -1.1785]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 90 of epoch 4 complete. Loss: 0.36706681879497227; Accuracy: 0.9; Time taken (s): 10.067210674285889\n",
      "O:  tensor([[-0.0902, -2.1770,  4.8359, -1.8272],\n",
      "        [ 3.2728, -2.1947,  0.1380,  0.7978],\n",
      "        [-0.6336,  3.6484, -2.1816, -0.0870]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 120 of epoch 4 complete. Loss: 0.17366934636738443; Accuracy: 0.9; Time taken (s): 10.074920415878296\n",
      "Epoch 4 complete! Development Accuracy: 0.7647058823529411; Development Loss: 1.0391924582149945\n",
      "O:  tensor([[ 0.8142,  3.5086, -2.6476, -1.4706],\n",
      "        [ 5.6903, -2.3338, -1.7292, -1.1443],\n",
      "        [ 5.2097,  0.1063, -2.3973, -2.7902]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 0 of epoch 5 complete. Loss: 0.044579104136310915; Accuracy: 1.0; Time taken (s): 8.893500804901123\n",
      "O:  tensor([[-0.8506, -2.2279, -2.3683,  3.6966],\n",
      "        [-0.3176, -2.0434, -2.2341,  3.7643],\n",
      "        [-0.1057, -3.7453,  4.5910, -1.7064]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 30 of epoch 5 complete. Loss: 0.029400960297488777; Accuracy: 1.0; Time taken (s): 10.731673240661621\n",
      "O:  tensor([[ 0.5140, -2.5775, -1.9462,  3.8385],\n",
      "        [ 1.8320, -3.4858, -2.1779,  3.3333],\n",
      "        [-1.3996, -1.0971,  4.5040, -1.4097]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 60 of epoch 5 complete. Loss: 0.270946867295189; Accuracy: 0.8; Time taken (s): 10.106551885604858\n",
      "O:  tensor([[-1.0523,  5.2512, -2.0542, -0.2942],\n",
      "        [-2.6170,  3.0944,  0.3429, -0.4140],\n",
      "        [ 4.7210, -3.0492, -0.2114, -0.3649]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 90 of epoch 5 complete. Loss: 0.030499531544714284; Accuracy: 1.0; Time taken (s): 10.112784624099731\n",
      "O:  tensor([[ 0.6845, -2.1805,  4.7373, -2.2931],\n",
      "        [ 4.1103, -3.0599, -0.5003, -0.8206],\n",
      "        [-0.1404,  3.8955, -1.9829, -1.3860]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 120 of epoch 5 complete. Loss: 0.016118929860621696; Accuracy: 1.0; Time taken (s): 10.122963666915894\n",
      "Epoch 5 complete! Development Accuracy: 0.7588235294117647; Development Loss: 1.0867551628641117\n",
      "O:  tensor([[ 1.8579,  4.1179, -2.2278, -1.7158],\n",
      "        [ 4.8929, -1.8933, -1.2425, -0.6702],\n",
      "        [ 5.0942, -0.7634, -2.1194, -1.5636]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 0 of epoch 6 complete. Loss: 0.32256549854892647; Accuracy: 0.8; Time taken (s): 8.71732759475708\n",
      "O:  tensor([[-0.6971, -2.9283, -2.5528,  4.7220],\n",
      "        [-0.7798, -1.5770, -2.0744,  3.9784],\n",
      "        [-1.2278, -2.2631,  4.9018, -2.0073]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 30 of epoch 6 complete. Loss: 0.06156027363840094; Accuracy: 1.0; Time taken (s): 10.591458559036255\n",
      "O:  tensor([[ 2.2044, -2.0170, -3.0649,  3.4488],\n",
      "        [ 0.7569, -2.2874, -4.0903,  3.7955],\n",
      "        [-1.0818, -0.8455,  4.2844, -2.5525]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 60 of epoch 6 complete. Loss: 0.3300090855946933; Accuracy: 0.9; Time taken (s): 10.073814392089844\n",
      "O:  tensor([[-2.1869,  5.6986, -2.3946, -1.2958],\n",
      "        [-2.0243,  5.8725, -1.0889, -1.8776],\n",
      "        [ 4.8156, -2.1170, -1.2397, -1.0882]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 90 of epoch 6 complete. Loss: 0.007981977076855327; Accuracy: 1.0; Time taken (s): 10.083214044570923\n",
      "O:  tensor([[-0.8968, -1.5822,  4.2654, -3.1810],\n",
      "        [ 4.2313, -0.9465,  0.1616, -2.1006],\n",
      "        [-0.0385,  4.5604, -2.9025, -2.9863]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 120 of epoch 6 complete. Loss: 0.06601265282135622; Accuracy: 1.0; Time taken (s): 10.085599184036255\n",
      "Epoch 6 complete! Development Accuracy: 0.779411764705882; Development Loss: 1.2596019009584387\n",
      "O:  tensor([[ 1.3458,  5.4488, -2.9667, -2.3311],\n",
      "        [ 6.1495, -3.2785, -1.4558, -0.7057],\n",
      "        [ 4.7755,  0.1201, -1.4788, -1.9828]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 0 of epoch 7 complete. Loss: 0.034204340073574806; Accuracy: 1.0; Time taken (s): 8.7969331741333\n",
      "O:  tensor([[ 0.9601, -2.3893, -2.1409,  2.9517],\n",
      "        [ 0.2165, -2.1872, -1.5044,  3.6417],\n",
      "        [ 1.0143, -2.3771,  2.8603, -1.5940]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 30 of epoch 7 complete. Loss: 0.060269098895283015; Accuracy: 1.0; Time taken (s): 10.631511449813843\n",
      "O:  tensor([[-0.9168, -2.2399, -3.4913,  6.1918],\n",
      "        [ 3.6926, -4.1161, -3.3750,  3.1513],\n",
      "        [-1.1522, -0.7667,  4.2980, -1.8420]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 60 of epoch 7 complete. Loss: 0.04075186919846251; Accuracy: 1.0; Time taken (s): 10.091970920562744\n",
      "O:  tensor([[-3.0929,  5.4638, -1.9498, -1.5583],\n",
      "        [-2.6466,  4.2737, -0.5799, -1.0333],\n",
      "        [ 4.6136, -3.5684,  0.1822, -1.4889]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 90 of epoch 7 complete. Loss: 0.009163826954085408; Accuracy: 1.0; Time taken (s): 10.078096866607666\n",
      "O:  tensor([[-2.0726, -1.0517,  5.6768, -0.9906],\n",
      "        [ 4.6886, -1.4216, -1.3260,  0.2126],\n",
      "        [-0.6270,  6.1963, -2.5081, -2.1161]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "L:  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0', dtype=torch.float64)\n",
      "Iteration 120 of epoch 7 complete. Loss: 0.015874477284480222; Accuracy: 1.0; Time taken (s): 10.13685131072998\n",
      "Epoch 7 complete! Development Accuracy: 0.7838235294117646; Development Loss: 1.1957215154604068\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 8\n",
    "labels = range(num_class_val)\n",
    "\n",
    "# train model\n",
    "class_weights = compute_class_weight(\"balanced\", classes=labels, y=train_veri_df.label.values)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).cuda()\n",
    "b_model = RelClassifier(num_class_val, dropout=0.5)\n",
    "opti = optim.Adam(b_model.parameters(), lr = 2e-5)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
    "gpu = 0\n",
    "\n",
    "# b_model.load_state_dict(torch.load(\"sstcls_0.dat\"))\n",
    "b_model.cuda(gpu) #Enable gpu support for the model\n",
    "train(b_model, criterion, opti, train_veri_dl, test_veri_dl, num_epoch, gpu, file_pre = \"val_\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 1, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 1, 0, 2, 0, 1, 2, 2, 0, 0, 1, 2, 0, 2, 0, 3, 2, 2, 0, 0, 2, 0, 0, 2, 0, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 2, 3, 2, 1, 2, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 2, 2, 0, 0, 0, 3, 1, 0, 2, 0, 0, 0, 0, 0, 1, 3, 0, 0, 1, 2, 0, 1, 0, 1, 2, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 1, 2, 0, 2, 3, 0, 2, 0, 2, 1, 1, 2, 0, 1, 1, 1, 2, 2, 0, 0, 0, 0, 2, 2, 2, 1, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 0, 2, 3, 0, 0, 2, 0, 3, 0, 0, 2, 3, 3, 1, 2, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 3, 2, 0, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 3, 0, 2, 0, 0, 3, 1, 2, 2, 0, 1, 2, 1, 0, 1, 2, 2, 1, 0, 2, 0, 2, 3, 0, 2, 0, 2, 2, 0, 1, 0, 2, 0, 0, 0, 2, 2, 0, 2, 0, 1, 0, 0, 1, 0, 1, 2, 0, 3, 2, 0, 2, 3, 0, 0, 2, 0, 0, 2, 1, 2, 0, 0, 0, 2, 3, 1, 2, 1, 3, 0, 2, 0, 1, 3, 2, 2, 1, 0, 0, 1, 2, 0, 2, 0, 1, 3, 2, 1, 0, 2, 0, 2, 1, 0, 0, 2, 2]\n",
      "[2, 0, 1, 0, 1, 2, 0, 0, 3, 0, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 1, 0, 2, 0, 2, 0, 2, 2, 0, 1, 2, 0, 2, 0, 3, 0, 2, 2, 0, 2, 2, 0, 2, 0, 2, 2, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 3, 2, 0, 2, 0, 2, 2, 3, 3, 2, 0, 0, 2, 0, 2, 2, 1, 1, 2, 3, 0, 0, 2, 3, 1, 2, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 3, 0, 0, 2, 2, 2, 0, 2, 2, 0, 0, 1, 1, 3, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 2, 3, 0, 2, 0, 0, 1, 3, 0, 2, 3, 0, 2, 3, 2, 1, 1, 2, 0, 1, 1, 0, 2, 2, 0, 3, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 0, 2, 3, 0, 0, 2, 2, 3, 0, 1, 2, 3, 3, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 0, 1, 0, 1, 2, 0, 3, 2, 2, 0, 0, 3, 1, 2, 2, 0, 1, 2, 1, 0, 0, 3, 0, 1, 0, 2, 0, 2, 3, 0, 2, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 0, 1, 2, 0, 3, 2, 0, 2, 3, 0, 0, 2, 0, 0, 2, 1, 2, 0, 1, 0, 2, 3, 1, 2, 0, 1, 0, 0, 0, 1, 3, 2, 2, 0, 0, 0, 1, 1, 0, 2, 0, 1, 3, 2, 1, 0, 0, 0, 3, 1, 0, 0, 0, 2]\n",
      "[[129  14   8   1]\n",
      " [ 11  36   1   3]\n",
      " [ 16   4  79   0]\n",
      " [  9   0   5  18]]\n",
      "0.7844311377245509\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## define prediction\n",
    "def predict_val(b_model, dataloader, gpu):\n",
    "    b_model.eval()\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, seg_li, _ in dataloader:\n",
    "            seq, attn_masks, seg_li = seq.cuda(gpu), attn_masks.cuda(gpu), seg_li.cuda(gpu)\n",
    "            output = b_model(seq, attn_masks, seg_li)\n",
    "\n",
    "            ## match label\n",
    "            pred += torch.argmax(output, dim=1).tolist()\n",
    "\n",
    "    return pred\n",
    "\n",
    "## train\n",
    "gpu = 0 \n",
    "true_label = test_veri_df.label.values.tolist()\n",
    "pred_label = predict_val(b_model, test_veri_dl, gpu)\n",
    "\n",
    "## Print out confusion matrix\n",
    "print(pred_label)\n",
    "print(true_label)\n",
    "print(confusion_matrix(true_label, pred_label))\n",
    "print(np.mean(np.array(true_label) == np.array(pred_label)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
