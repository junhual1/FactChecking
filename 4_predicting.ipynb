{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 + NER + BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 23:50:16.437049: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-14 23:50:16.660449: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-14 23:50:17.779172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-05-14 23:50:19.411804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-14 23:50:19.437684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-14 23:50:19.438053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import pandas as pd\n",
    "import string\n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "# TRAIN_PATH = \"./data/train-claims.json\"\n",
    "# TRAIN_PATH = \"./data/dev-claims.json\"\n",
    "TRAIN_PATH = \"./data/test-claims-unlabelled.json\"\n",
    "EVI_PATH = \"./data/evidence.json\"\n",
    "NER_PATH = \"./data/spacy_ner_tot.json\" ## MUST\n",
    "VAL_MODEL_PATH = \"./val_model.dat\" ## MUST\n",
    "REL_MODEL_PATH = \"./rela_model.dat\" ## MUST\n",
    "BM25_PATH = \"./data/bm_claim_test.pkl\" ## Producable\n",
    "VERI_DF_PATH = './data/train_filt.pkl'\n",
    "\n",
    "PAD_LEN = 50\n",
    "RANDOM_SEED = 42\n",
    "SUPPORTS = \"SUPPORTS\"\n",
    "REFUTES = \"REFUTES\"\n",
    "NOT_ENOUGH_INFO = \"NOT_ENOUGH_INFO\"\n",
    "DISPUTED = \"DISPUTED\"\n",
    "RELATED = 1\n",
    "NOT_RELATED = 0\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "puncu_remove = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(input):\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(input.lower().translate(puncu_remove))\n",
    "    # remove stop words, lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(i) for i in tokens if i not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def get_n_bm25(bm25, text, n):\n",
    "    # Retrive and select top n bm25 score\n",
    "    token = preprocess(text)\n",
    "    scores = bm25.get_scores(token)\n",
    "    scores = np.array([i for i in enumerate(scores)])\n",
    "    top_n_indices = [[int(i[0]), float(i[1])] for i in scores[scores[:, 1].argsort()][-n:]]\n",
    "    print([i[0] for i in top_n_indices])\n",
    "    return [i[0] for i in top_n_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_id_df = pd.read_json(TRAIN_PATH, orient=\"index\")\n",
    "evi_id_df = pd.read_json(EVI_PATH, orient=\"index\").rename({0: \"evi_text\"}, axis=1)\n",
    "claim_df = claim_id_df.reset_index(names=\"claim\")\n",
    "evi_df = evi_id_df.reset_index(names=\"evidence\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MB25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 43 min\n",
    "# max_evi = 10\n",
    "\n",
    "# ## preparing MB25 ranking\n",
    "# with tf.device('/GPU:0'):\n",
    "#     document = evi_df[\"evi_text\"].values\n",
    "#     document = [preprocess(i) for i in document]\n",
    "#     bm25 = BM25Okapi(document)\n",
    "\n",
    "# ## rank and find top n for each claim\n",
    "# with tf.device('/GPU:0'):\n",
    "#     pred_df = claim_df[[\"claim\", \"claim_text\"]]\n",
    "#     # pred_df = claim_df[[\"claim\", \"claim_text\", \"claim_label\", \"evidences\"]]\n",
    "#     pred_df[\"pred_evidence\"] = pred_df.apply(lambda x: [evi_df.loc[i][\"evidence\"] for i in get_n_bm25(bm25, x[\"claim_text\"], max_evi)], axis=1)\n",
    "#     print(pred_df.head(1))\n",
    "\n",
    "# pred_df.to_pickle(BM25_PATH)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MB25 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.read_pickle(BM25_PATH)\n",
    "\n",
    "# tot_cr = 0\n",
    "# tot_np = 0\n",
    "# tot_more = 0\n",
    "# for true_e, pred in pred_df[[\"evidences\", \"pred_evidence\"]].values:\n",
    "#     cur_cr = 0\n",
    "#     cur_more = 0\n",
    "#     for evi in pred:\n",
    "#         if evi in true_e:\n",
    "#             cur_cr += 1\n",
    "#         else:\n",
    "#             cur_more += 1\n",
    "#     cur_np = len(true_e) - cur_cr\n",
    "#     tot_cr += cur_cr\n",
    "#     tot_np += cur_np\n",
    "#     tot_more += cur_more\n",
    "\n",
    "# print(\"True Positive:\", tot_cr)\n",
    "# print(\"False Positive:\", tot_more)\n",
    "# print(\"False Negative:\", tot_np)\n",
    "# print(\"Percentage Retrieval: \", tot_cr / (tot_cr + tot_np))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read premade NER file\n",
    "with open(NER_PATH, \"r\") as cur_file:\n",
    "        ner_dict = json.loads(cur_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        claim         evidence\n",
      "0  claim-1003  evidence-561136\n"
     ]
    }
   ],
   "source": [
    "max_evi = 0\n",
    "ner_hurdle = 500\n",
    "\n",
    "\n",
    "ner_df = pd.DataFrame(columns=[\"claim\", \"evidence\"])\n",
    "\n",
    "## Matching NER in claim with NER in evidence\n",
    "for cur_id, cur_text in claim_df[[\"claim\", \"claim_text\"]].values[:100]:\n",
    "    doc = spacy_nlp(cur_text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ner_dict and ent.text.lower().translate(puncu_remove) in ner_dict[ent.label_]:\n",
    "            if len(ner_dict[ent.label_][ent.text.lower().translate(puncu_remove)]) > ner_hurdle: continue\n",
    "            for evi in ner_dict[ent.label_][ent.text.lower().translate(puncu_remove)]:\n",
    "                ner_df = ner_df.append({\"claim\": cur_id, \"evidence\":evi}, ignore_index=True)\n",
    "            \n",
    "print(ner_df.head(1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ner_dict = defaultdict(list)\n",
    "# ner_ev_df = pd.DataFrame(columns=[\"claim\", \"evidences\", \"pred_evidence\"])\n",
    "\n",
    "# for record in ner_df.values:\n",
    "#     ner_dict[record[0]].append(record[1])\n",
    "\n",
    "# for record in claim_df.values:\n",
    "#     if record[0] in ner_dict:\n",
    "#         ner_ev_df = ner_ev_df.append({\"claim\": record[0], \"pred_evidence\": ner_dict[record[0]], \"evidences\": record[3]}, ignore_index = True)\n",
    "#     else:\n",
    "#         ner_ev_df = ner_ev_df.append({\"claim\": record[0], \"pred_evidence\": [], \"evidences\": record[3]}, ignore_index = True)\n",
    "\n",
    "# tot_cr = 0\n",
    "# tot_np = 0\n",
    "# tot_more = 0\n",
    "# for true_e, pred in ner_ev_df[[\"evidences\", \"pred_evidence\"]].values:\n",
    "#     cur_cr = 0\n",
    "#     cur_more = 0\n",
    "#     for evi in pred:\n",
    "#         if evi in true_e:\n",
    "#             cur_cr += 1\n",
    "#         else:\n",
    "#             cur_more += 1\n",
    "#     cur_np = len(true_e) - cur_cr\n",
    "#     tot_cr += cur_cr\n",
    "#     tot_np += cur_np\n",
    "#     tot_more += cur_more\n",
    "\n",
    "# print(\"True Positive:\", tot_cr)\n",
    "# print(\"False Positive:\", tot_more)\n",
    "# print(\"False Negative:\", tot_np)\n",
    "# print(\"Percentage Retrieval: \", tot_cr / (tot_cr + tot_np))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing for bert model, concatenanting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        claim_id       evidence_id\n",
      "0     claim-1001   evidence-334722\n",
      "1     claim-1001   evidence-570367\n",
      "2     claim-1001   evidence-445405\n",
      "3     claim-1001    evidence-67154\n",
      "4     claim-1001   evidence-360246\n",
      "...          ...               ...\n",
      "8878  claim-2840  evidence-1087946\n",
      "8879  claim-2840  evidence-1116441\n",
      "8880  claim-2840  evidence-1144991\n",
      "8881  claim-2840  evidence-1160433\n",
      "8882  claim-2840  evidence-1170600\n",
      "\n",
      "[8883 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "rel_df = pd.DataFrame(columns=[\"claim\", \"evidence\"])\n",
    "for claim, evidence in pred_df[[\"claim\", \"pred_evidence\"]].values:\n",
    "    for evi in evidence:\n",
    "        rel_df = rel_df.append({\"claim\": claim, \"evidence\":evi}, ignore_index=True)\n",
    "rel_df = pd.concat([rel_df, ner_df])\n",
    "rel_df = rel_df.reset_index(drop=True).rename({\"claim\": \"claim_id\", \"evidence\": \"evidence_id\"}, axis=1)\n",
    "print(rel_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim_id', 'evidence_id', 'claim', 'evidence', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "rel_df['claim'] = rel_df.apply(lambda x: claim_id_df.loc[x[\"claim_id\"]][\"claim_text\"], axis=1)\n",
    "rel_df['evidence'] = rel_df.apply(lambda x: evi_id_df.loc[x[\"evidence_id\"]][\"evi_text\"], axis=1)\n",
    "rel_df[\"label\"] = 0\n",
    "\n",
    "print(rel_df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "PAD = \"[PAD]\"\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "PAD_LEN = 50\n",
    "RANDOM_SEED = 42\n",
    "gpu = 0\n",
    "SUPPORTS = \"SUPPORTS\"\n",
    "REFUTES = \"REFUTES\"\n",
    "NOT_ENOUGH_INFO = \"NOT_ENOUGH_INFO\"\n",
    "DISPUTED = \"DISPUTED\"\n",
    "RELATED = 1\n",
    "NOT_RELATED = 0\n",
    "label_trans = {SUPPORTS: 0, REFUTES: 1, NOT_ENOUGH_INFO: 2, DISPUTED: 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RelDataset(Dataset):\n",
    "    def __init__(self, input_df, max_len_claim, max_len_evi, num_class):\n",
    "        self.df = input_df\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_len_claim = max_len_claim\n",
    "        self.max_len_evi = max_len_evi\n",
    "        self.num_class = num_class\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ## claim_text, claim_label, evidences\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        claim = self.df.loc[index, 'claim']\n",
    "        label = self.df.loc[index, 'label']\n",
    "        evidence = self.df.loc[index, 'evidence']\n",
    "\n",
    "        #Preprocessing the claim\n",
    "        claim = self.tokenizer.tokenize(claim)\n",
    "        evidence = self.tokenizer.tokenize(evidence)\n",
    "        claim = pad_sen(claim, self.max_len_claim)\n",
    "        evidence = pad_sen(evidence, self.max_len_evi)\n",
    "        input_token, seg_li = saperate_evi(claim, evidence)\n",
    "        attn_mask = [1 if token != PAD else 0 for token in input_token]\n",
    "        input_token = self.tokenizer.convert_tokens_to_ids(input_token)\n",
    "        input_id = torch.tensor(input_token) #Converting the list to a pytorch tensor\n",
    "        attn_mask = torch.tensor(attn_mask)\n",
    "        seg_li = torch.tensor(seg_li)\n",
    "        labels = np.zeros(self.num_class)\n",
    "        np.put(labels, label, 1)\n",
    "        label = torch.tensor(labels)\n",
    "        \n",
    "        # print(input_token, attn_mask, seg_li)\n",
    "        \n",
    "        return input_id, attn_mask, seg_li, label\n",
    "\n",
    "def pad_sen(input, max_len):\n",
    "    ## Add padding to sequence\n",
    "    if len(input) > max_len: return input[:max_len]\n",
    "    return input+[PAD for _ in range(max_len-len(input))]\n",
    "\n",
    "def saperate_evi(claim, evidence):\n",
    "    ## seperate and concatenate claim and evidence\n",
    "    output = [CLS] + claim + [SEP] + evidence + [SEP]\n",
    "    seg_li = [0 for _ in range(len(claim)+2)] + [1 for _ in range(len(evidence)+1)]\n",
    "    return output, seg_li\n",
    "\n",
    "class RelClassifier(nn.Module):\n",
    "    def __init__(self, num_class, dropout = 0.1):\n",
    "        super(RelClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertForSequenceClassification.from_pretrained('bert-base-uncased', problem_type=\"multi_label_classification\")\n",
    "        \n",
    "        #Classification layer\n",
    "        #input dimension is 768 because [CLS] embedding has a dimension of 768\n",
    "        self.linear = nn.Linear(768, num_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg_li):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "        \n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, token_type_ids = seg_li, output_hidden_states=True)\n",
    "        cont_reps = outputs.hidden_states[-1][:,0]\n",
    "        dropout_output = self.dropout(cont_reps)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        return linear_output\n",
    "    \n",
    "def get_accuracy(output, labels):\n",
    "    # return accuracy of the prediction based on class score\n",
    "    return (output.argmax(dim=1) == labels.argmax(dim=1)).sum().item() / len(labels)\n",
    "\n",
    "def evaluate(b_model, criterion, dataloader, gpu):\n",
    "    ## evaluate accuracy and loss\n",
    "    b_model.eval()\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, seg_li, labels in dataloader:\n",
    "            seq, attn_masks, seg_li, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_li.cuda(gpu), labels.cuda(gpu)\n",
    "            output = b_model(seq, attn_masks, seg_li).cuda(gpu)\n",
    "            mean_loss += criterion(output, labels)\n",
    "            mean_acc += get_accuracy(output, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count\n",
    "\n",
    "def train(b_model, criterion, opti, train_loader, dev_loader, max_eps, gpu, file_pre = \"model_\"):\n",
    "    ## Train model\n",
    "    best_acc = 0\n",
    "    best_loss = 99\n",
    "    st = time.time()\n",
    "    for ep in range(max_eps):\n",
    "        \n",
    "        b_model.train()\n",
    "        for it, (seq, attn_masks, seg_li, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, seg_li, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_li.cuda(gpu), labels.cuda(gpu)\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            output = b_model(seq, attn_masks, seg_li)\n",
    "            \n",
    "            #Computing loss\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "            \n",
    "            ## print\n",
    "            if it % 30 == 0:\n",
    "                acc = get_accuracy(output, labels)\n",
    "                print(\"O: \", output[:3])\n",
    "                print(\"L: \", labels[:3])\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "        \n",
    "        ## save best model\n",
    "        dev_acc, dev_loss = evaluate(b_model, criterion, dev_loader, gpu)\n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
    "        if dev_acc > best_acc and dev_loss <= best_loss:\n",
    "            print(\"Accuracy improved from {} to {}, Loss improved from {} to {}, saving model...\".format(best_acc, dev_acc, best_loss, dev_loss))\n",
    "            print(\"Saved: \" + file_pre + 'model.dat')\n",
    "            best_acc = dev_acc\n",
    "            best_loss = dev_loss\n",
    "            torch.save(b_model.state_dict(), file_pre + 'model.dat')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RelClassifier(\n",
       "  (bert_layer): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_CLAIM = 64\n",
    "PAD_EVIDENCE = 64\n",
    "BATCH_SIZE = 32\n",
    "NO_WORKER = 4\n",
    "num_class_rel = 2\n",
    "gpu = 0 #gpu ID\n",
    "\n",
    "\n",
    "## load dataset and model\n",
    "rel_ds = RelDataset(rel_df[['claim', 'evidence', 'label']], PAD_CLAIM, PAD_EVIDENCE, num_class_rel)\n",
    "rel_dl = DataLoader(rel_ds, batch_size = BATCH_SIZE, num_workers = NO_WORKER)\n",
    "rel_mod = RelClassifier(num_class_rel)\n",
    "rel_mod.load_state_dict(torch.load(REL_MODEL_PATH))\n",
    "rel_mod.cuda(gpu) #Enable gpu support for the model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rel(net, dataloader, gpu):\n",
    "    net.eval()\n",
    "    pred = []\n",
    "    max_score = []\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, seg_li, _ in dataloader:\n",
    "            seq, attn_masks, seg_li = seq.cuda(gpu), attn_masks.cuda(gpu), seg_li.cuda(gpu)\n",
    "            output = net(seq, attn_masks, seg_li)\n",
    "\n",
    "            for record in output:\n",
    "                pred.append(record.argmax().item())\n",
    "                max_score.append(record.max().item())\n",
    "\n",
    "    return pred, max_score\n",
    "\n",
    "gpu = 0 #gpu ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label, log_score = predict_rel(rel_mod, rel_dl, gpu)\n",
    "\n",
    "\n",
    "rel_df[\"score\"] = log_score\n",
    "rel_df[\"pred_label\"] = pred_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformate for next training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'claim-1001': ['evidence-895046', 'evidence-277435'], 'claim-1003': ['evidence-829081', 'evidence-430670', 'evidence-18442'], 'claim-1009': ['evidence-482300', 'evidence-649967', 'evidence-74163'], 'claim-1020': ['evidence-382341', 'evidence-754568', 'evidence-195294'], 'claim-1028': ['evidence-18609', 'evidence-154614', 'evidence-644769'], 'claim-1034': ['evidence-572246', 'evidence-23786', 'evidence-164796'], 'claim-1048': ['evidence-970539', 'evidence-508793', 'evidence-735431'], 'claim-109': ['evidence-4318', 'evidence-508668', 'evidence-384007'], 'claim-1135': ['evidence-1198124', 'evidence-647256', 'evidence-265967'], 'claim-1141': ['evidence-1139877', 'evidence-591257', 'evidence-591257'], 'claim-1156': ['evidence-1142116', 'evidence-78739', 'evidence-409638'], 'claim-1173': ['evidence-987493', 'evidence-666596', 'evidence-127142'], 'claim-1202': ['evidence-107116'], 'claim-1212': ['evidence-697698', 'evidence-594802', 'evidence-86066'], 'claim-1230': ['evidence-974738', 'evidence-482838', 'evidence-724993'], 'claim-1237': ['evidence-352802', 'evidence-373203', 'evidence-370562'], 'claim-1240': ['evidence-954212', 'evidence-289800', 'evidence-731434'], 'claim-1243': ['evidence-286845', 'evidence-731434', 'evidence-954212'], 'claim-1271': ['evidence-1099491', 'evidence-173281', 'evidence-668284'], 'claim-1304': ['evidence-3767', 'evidence-865226', 'evidence-146739'], 'claim-1307': ['evidence-513559', 'evidence-629924', 'evidence-828180'], 'claim-1343': ['evidence-653981', 'evidence-652327', 'evidence-898741'], 'claim-1351': ['evidence-356178', 'evidence-1071200'], 'claim-1378': ['evidence-116639', 'evidence-1062826', 'evidence-588787'], 'claim-138': ['evidence-921916', 'evidence-1010541'], 'claim-1458': ['evidence-353546', 'evidence-130771', 'evidence-616523'], 'claim-1475': ['evidence-592080', 'evidence-519084', 'evidence-553190'], 'claim-148': ['evidence-576103', 'evidence-37274', 'evidence-239881'], 'claim-1488': ['evidence-254130', 'evidence-765914', 'evidence-760483'], 'claim-1508': ['evidence-1092088', 'evidence-196453', 'evidence-842114'], 'claim-1531': ['evidence-137421'], 'claim-1560': ['evidence-65928', 'evidence-1076062', 'evidence-255466'], 'claim-1582': ['evidence-902330'], 'claim-1588': ['evidence-817308', 'evidence-903119', 'evidence-240134'], 'claim-1604': ['evidence-1049076', 'evidence-381399', 'evidence-499734'], 'claim-1609': ['evidence-1200633', 'evidence-52981', 'evidence-885328'], 'claim-1652': ['evidence-137437', 'evidence-98914', 'evidence-386828'], 'claim-1672': ['evidence-118792', 'evidence-547830', 'evidence-922640'], 'claim-1684': ['evidence-22380', 'evidence-818530', 'evidence-990389'], 'claim-1691': ['evidence-169769', 'evidence-665226', 'evidence-275934'], 'claim-1761': ['evidence-104566', 'evidence-979052', 'evidence-66273'], 'claim-1771': ['evidence-28478', 'evidence-1197833', 'evidence-417371'], 'claim-1842': ['evidence-102598', 'evidence-570779', 'evidence-570779'], 'claim-1853': ['evidence-653867', 'evidence-59174', 'evidence-972641'], 'claim-1872': ['evidence-485836', 'evidence-660624', 'evidence-235172'], 'claim-1908': ['evidence-1065464', 'evidence-796018', 'evidence-351939'], 'claim-1977': ['evidence-803650', 'evidence-118226', 'evidence-1090815'], 'claim-1985': ['evidence-1133373', 'evidence-1062078', 'evidence-1062078'], 'claim-1998': ['evidence-975948', 'evidence-772442', 'evidence-35298'], 'claim-2013': ['evidence-1192475', 'evidence-34411', 'evidence-886517'], 'claim-2028': ['evidence-589361', 'evidence-1178287'], 'claim-2084': ['evidence-1047356', 'evidence-351113'], 'claim-21': ['evidence-520022', 'evidence-732643'], 'claim-2105': ['evidence-874535', 'evidence-142343', 'evidence-1100614'], 'claim-2167': ['evidence-725190', 'evidence-828165'], 'claim-2204': ['evidence-814721', 'evidence-51798'], 'claim-2209': ['evidence-716813', 'evidence-924446', 'evidence-284584'], 'claim-2219': ['evidence-949961', 'evidence-927146', 'evidence-789440'], 'claim-2246': ['evidence-948375', 'evidence-950248', 'evidence-595828'], 'claim-2248': ['evidence-349531', 'evidence-412505', 'evidence-968110'], 'claim-2329': ['evidence-866245', 'evidence-1054833', 'evidence-738340'], 'claim-2347': ['evidence-727479', 'evidence-260640'], 'claim-2398': ['evidence-499734', 'evidence-625922', 'evidence-66273'], 'claim-2411': ['evidence-187806', 'evidence-419359'], 'claim-2423': ['evidence-253690', 'evidence-1170600', 'evidence-877881'], 'claim-2427': ['evidence-814790', 'evidence-338226', 'evidence-1147406'], 'claim-2428': ['evidence-80695', 'evidence-1205623', 'evidence-489555'], 'claim-2434': ['evidence-948375', 'evidence-950248', 'evidence-371208'], 'claim-2464': ['evidence-927438', 'evidence-8189'], 'claim-2468': ['evidence-597084', 'evidence-801082', 'evidence-185218'], 'claim-2476': ['evidence-728409', 'evidence-513478', 'evidence-1016008'], 'claim-2509': ['evidence-984887', 'evidence-32728', 'evidence-32728'], 'claim-2561': ['evidence-950248', 'evidence-902107', 'evidence-363579'], 'claim-2564': ['evidence-719962', 'evidence-758479'], 'claim-2575': ['evidence-9637', 'evidence-479543', 'evidence-888119'], 'claim-2590': ['evidence-507814', 'evidence-798804', 'evidence-652349'], 'claim-2599': ['evidence-831631', 'evidence-814761', 'evidence-1175545'], 'claim-2631': ['evidence-600294', 'evidence-600294', 'evidence-600294'], 'claim-2733': ['evidence-1114252', 'evidence-152168', 'evidence-771272'], 'claim-275': ['evidence-922981', 'evidence-1127182', 'evidence-624715'], 'claim-2751': ['evidence-1202332', 'evidence-944007', 'evidence-960067'], 'claim-2754': ['evidence-977922', 'evidence-7424', 'evidence-202927'], 'claim-2755': ['evidence-415162', 'evidence-603669', 'evidence-273182'], 'claim-2774': ['evidence-322438', 'evidence-972542', 'evidence-606800'], 'claim-2783': ['evidence-1095868', 'evidence-1104389'], 'claim-2797': ['evidence-1000246', 'evidence-217100', 'evidence-975125'], 'claim-28': ['evidence-799984', 'evidence-106742', 'evidence-1127398'], 'claim-2815': ['evidence-1022914', 'evidence-890117', 'evidence-890931'], 'claim-2840': ['evidence-591257', 'evidence-1170600', 'evidence-932068'], 'claim-2857': ['evidence-199520', 'evidence-972046', 'evidence-1109298'], 'claim-2870': ['evidence-1038383', 'evidence-316670'], 'claim-293': ['evidence-166447'], 'claim-2942': ['evidence-923136', 'evidence-801082', 'evidence-661699'], 'claim-2951': ['evidence-1089445', 'evidence-491726', 'evidence-325383'], 'claim-2967': ['evidence-963856', 'evidence-308923', 'evidence-43606'], 'claim-2977': ['evidence-1173070', 'evidence-216481', 'evidence-427368'], 'claim-298': ['evidence-899905', 'evidence-877733', 'evidence-352729'], 'claim-2994': ['evidence-955425', 'evidence-489159', 'evidence-457314'], 'claim-30': ['evidence-780092', 'evidence-975948', 'evidence-870727'], 'claim-3038': ['evidence-1006052', 'evidence-203533', 'evidence-915569'], 'claim-3064': ['evidence-1196519', 'evidence-240414', 'evidence-98842'], 'claim-3072': ['evidence-1196020', 'evidence-725977', 'evidence-918450'], 'claim-3074': ['evidence-957572', 'evidence-526818', 'evidence-483878'], 'claim-3119': ['evidence-950186', 'evidence-556098', 'evidence-652818'], 'claim-3123': ['evidence-725977', 'evidence-100461', 'evidence-666859'], 'claim-3127': ['evidence-587522', 'evidence-1166652', 'evidence-15680'], 'claim-338': ['evidence-240308', 'evidence-196453', 'evidence-842114'], 'claim-350': ['evidence-210522'], 'claim-381': ['evidence-903568', 'evidence-457668', 'evidence-438554'], 'claim-400': ['evidence-880073', 'evidence-67402'], 'claim-404': ['evidence-801082', 'evidence-1006052', 'evidence-627472'], 'claim-454': ['evidence-274044', 'evidence-679275', 'evidence-429536'], 'claim-467': ['evidence-402691', 'evidence-332166'], 'claim-474': ['evidence-438158', 'evidence-1172097', 'evidence-1013303'], 'claim-477': ['evidence-425896', 'evidence-1108706', 'evidence-1128272'], 'claim-494': ['evidence-1131498'], 'claim-503': ['evidence-493422', 'evidence-680290', 'evidence-7898'], 'claim-520': ['evidence-116371', 'evidence-864147', 'evidence-1126413'], 'claim-532': ['evidence-617746', 'evidence-465018', 'evidence-778407'], 'claim-539': ['evidence-953997', 'evidence-338219', 'evidence-801457'], 'claim-556': ['evidence-70141', 'evidence-917316', 'evidence-401833'], 'claim-616': ['evidence-276438', 'evidence-999756', 'evidence-462231'], 'claim-678': ['evidence-738040'], 'claim-712': ['evidence-786173', 'evidence-756019'], 'claim-763': ['evidence-879668', 'evidence-674731', 'evidence-912487'], 'claim-770': ['evidence-1005062', 'evidence-622225', 'evidence-111473'], 'claim-812': ['evidence-712376', 'evidence-1133936', 'evidence-457809'], 'claim-822': ['evidence-50384', 'evidence-1106692'], 'claim-839': ['evidence-1150951', 'evidence-1145961', 'evidence-236630'], 'claim-893': ['evidence-595183', 'evidence-98803', 'evidence-312849'], 'claim-903': ['evidence-75555', 'evidence-12425', 'evidence-561302'], 'claim-910': ['evidence-117114', 'evidence-197221', 'evidence-715513'], 'claim-952': ['evidence-705706', 'evidence-855683', 'evidence-288351'], 'claim-972': ['evidence-22100', 'evidence-450316', 'evidence-620559'], 'claim-979': ['evidence-957389', 'evidence-434312', 'evidence-178433'], 'claim-1425': ['evidence-1116814', 'evidence-1117775', 'evidence-19423'], 'claim-2838': ['evidence-781432', 'evidence-308147']})\n"
     ]
    }
   ],
   "source": [
    "SCORE_HEARDLE = 1\n",
    "MAX_EVI = 3\n",
    "\n",
    "## Selet evidence selection\n",
    "train_df = rel_df.loc[rel_df[\"score\"] > SCORE_HEARDLE]\n",
    "train_df = train_df.loc[train_df[\"pred_label\"] == RELATED]\n",
    "train_df = train_df.drop(\"pred_label\", axis = 1)\n",
    "top_dict = defaultdict(list)\n",
    "\n",
    "## Sep dataframe into dictionary and merge\n",
    "for record in train_df.values:\n",
    "    top_dict[record[0]].append((record[5], record[1]))\n",
    "\n",
    "for key in top_dict.keys():\n",
    "    candi = sorted(top_dict[key], reverse = True)\n",
    "    candi = candi[:MAX_EVI] if len(candi) > MAX_EVI else candi\n",
    "    candi = [i[-1] for i in candi]\n",
    "    top_dict[key] = candi\n",
    "\n",
    "print(top_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Convert dictionary to dataframe\n",
    "# veri_df = pd.DataFrame(columns = [\"claim\", \"evidence\", \"label\", \"evidence_id\", \"claim_id\"])\n",
    "veri_df = pd.DataFrame(columns = [\"claim\", \"evidence\", \"evidence_id\", \"claim_id\"])\n",
    "\n",
    "for key in top_dict.keys():\n",
    "    claim = claim_id_df.loc[key].values[0]\n",
    "    # label = claim_id_df.loc[key].values[1]\n",
    "    evidence = [evi_id_df.loc[i].values[0] for i in top_dict[key]]\n",
    "    # veri_df = veri_df.append({\"claim\": claim, \"evidence\": evidence, \"label\": label, \"evidence_id\": top_dict[key], \"claim_id\": key}, ignore_index=True)\n",
    "    veri_df = veri_df.append({\"claim\": claim, \"evidence\": evidence, \"evidence_id\": top_dict[key], \"claim_id\": key}, ignore_index=True)\n",
    "\n",
    "veri_df.to_pickle(VERI_DF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>evidence_id</th>\n",
       "      <th>claim_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‘This study goes beyond statistical correlatio...</td>\n",
       "      <td>[Correlations have been identified between hig...</td>\n",
       "      <td>[evidence-895046, evidence-277435]</td>\n",
       "      <td>claim-1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A recent study in Nature Geoscience, for insta...</td>\n",
       "      <td>[Since the last glacial maximum about 20,000 y...</td>\n",
       "      <td>[evidence-829081, evidence-430670, evidence-18...</td>\n",
       "      <td>claim-1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‘Arctic ice conditions have been tracking at r...</td>\n",
       "      <td>[Arctic sea ice extent ice hit an all-time low...</td>\n",
       "      <td>[evidence-482300, evidence-649967, evidence-74...</td>\n",
       "      <td>claim-1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“The global reef crisis does not necessarily m...</td>\n",
       "      <td>[With widespread degradation of highly biodive...</td>\n",
       "      <td>[evidence-382341, evidence-754568, evidence-19...</td>\n",
       "      <td>claim-1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A second coat of paint has much less of an eff...</td>\n",
       "      <td>[A dog 's coat may be a double coat, made up o...</td>\n",
       "      <td>[evidence-18609, evidence-154614, evidence-644...</td>\n",
       "      <td>claim-1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>The Alaskan tundra is warming so quickly it ha...</td>\n",
       "      <td>[Recent warming is followed by carbon dioxide ...</td>\n",
       "      <td>[evidence-705706, evidence-855683, evidence-28...</td>\n",
       "      <td>claim-952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>“Arctic land stores about twice as much carbon...</td>\n",
       "      <td>[Both the decay and the burning of wood releas...</td>\n",
       "      <td>[evidence-22100, evidence-450316, evidence-620...</td>\n",
       "      <td>claim-972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>“Warm weather worsened the most recent five-ye...</td>\n",
       "      <td>[Between 2011 and 2014, California experienced...</td>\n",
       "      <td>[evidence-957389, evidence-434312, evidence-17...</td>\n",
       "      <td>claim-979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Each year sees the disappearance of thousands ...</td>\n",
       "      <td>[In August 2002 a flood caused by over a week ...</td>\n",
       "      <td>[evidence-1116814, evidence-1117775, evidence-...</td>\n",
       "      <td>claim-1425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>The data is being reported by the University o...</td>\n",
       "      <td>[The precipitation threshold (in millimetres) ...</td>\n",
       "      <td>[evidence-781432, evidence-308147]</td>\n",
       "      <td>claim-2838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 claim  \\\n",
       "0    ‘This study goes beyond statistical correlatio...   \n",
       "1    A recent study in Nature Geoscience, for insta...   \n",
       "2    ‘Arctic ice conditions have been tracking at r...   \n",
       "3    “The global reef crisis does not necessarily m...   \n",
       "4    A second coat of paint has much less of an eff...   \n",
       "..                                                 ...   \n",
       "132  The Alaskan tundra is warming so quickly it ha...   \n",
       "133  “Arctic land stores about twice as much carbon...   \n",
       "134  “Warm weather worsened the most recent five-ye...   \n",
       "135  Each year sees the disappearance of thousands ...   \n",
       "136  The data is being reported by the University o...   \n",
       "\n",
       "                                              evidence  \\\n",
       "0    [Correlations have been identified between hig...   \n",
       "1    [Since the last glacial maximum about 20,000 y...   \n",
       "2    [Arctic sea ice extent ice hit an all-time low...   \n",
       "3    [With widespread degradation of highly biodive...   \n",
       "4    [A dog 's coat may be a double coat, made up o...   \n",
       "..                                                 ...   \n",
       "132  [Recent warming is followed by carbon dioxide ...   \n",
       "133  [Both the decay and the burning of wood releas...   \n",
       "134  [Between 2011 and 2014, California experienced...   \n",
       "135  [In August 2002 a flood caused by over a week ...   \n",
       "136  [The precipitation threshold (in millimetres) ...   \n",
       "\n",
       "                                           evidence_id    claim_id  \n",
       "0                   [evidence-895046, evidence-277435]  claim-1001  \n",
       "1    [evidence-829081, evidence-430670, evidence-18...  claim-1003  \n",
       "2    [evidence-482300, evidence-649967, evidence-74...  claim-1009  \n",
       "3    [evidence-382341, evidence-754568, evidence-19...  claim-1020  \n",
       "4    [evidence-18609, evidence-154614, evidence-644...  claim-1028  \n",
       "..                                                 ...         ...  \n",
       "132  [evidence-705706, evidence-855683, evidence-28...   claim-952  \n",
       "133  [evidence-22100, evidence-450316, evidence-620...   claim-972  \n",
       "134  [evidence-957389, evidence-434312, evidence-17...   claim-979  \n",
       "135  [evidence-1116814, evidence-1117775, evidence-...  claim-1425  \n",
       "136                 [evidence-781432, evidence-308147]  claim-2838  \n",
       "\n",
       "[137 rows x 4 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veri_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieval bert relevance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rel_bert_dict = defaultdict(list)\n",
    "# bert_ev_df = pd.DataFrame(columns=[\"claim\", \"evidences\", \"pred_evidence\"])\n",
    "\n",
    "# for record in veri_df.values:\n",
    "#     rel_bert_dict[record[4]] = record[3]\n",
    "\n",
    "# for record in claim_df.values:\n",
    "#     if record[0] in rel_bert_dict:\n",
    "#         bert_ev_df = bert_ev_df.append({\"claim\": record[0], \"pred_evidence\": rel_bert_dict[record[0]], \"evidences\": record[3]}, ignore_index = True)\n",
    "#     else:\n",
    "#         bert_ev_df = bert_ev_df.append({\"claim\": record[0], \"pred_evidence\": [], \"evidences\": record[3]}, ignore_index = True)\n",
    "\n",
    "# tot_cr = 0\n",
    "# tot_np = 0\n",
    "# tot_more = 0\n",
    "# for true_e, pred in bert_ev_df[[\"evidences\", \"pred_evidence\"]].values:\n",
    "#     cur_cr = 0\n",
    "#     cur_more = 0\n",
    "#     for evi in pred:\n",
    "#         if evi in true_e:\n",
    "#             cur_cr += 1\n",
    "#         else:\n",
    "#             cur_more += 1\n",
    "#     cur_np = len(true_e) - cur_cr\n",
    "#     tot_cr += cur_cr\n",
    "#     tot_np += cur_np\n",
    "#     tot_more += cur_more\n",
    "\n",
    "# print(\"True Positive:\", tot_cr)\n",
    "# print(\"False Positive:\", tot_more)\n",
    "# print(\"False Negative:\", tot_np)\n",
    "# print(\"Percentage Retrieval: \", tot_cr / (tot_cr + tot_np))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claim verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_sep(claim, evidence, max_evi):\n",
    "    ## inputs are tokens\n",
    "    output = [CLS] + claim + [SEP]\n",
    "    seg_li = [0 for _ in range(len(claim)+2)]\n",
    "    cur_seg = 1\n",
    "    for evi in evidence:\n",
    "        output += evi + [SEP]\n",
    "        seg_li += [cur_seg for _ in range(len(evi)+1)]\n",
    "        # cur_seg += 1\n",
    "    for i in range(len(evidence), max_evi):\n",
    "        output += [PAD for _ in range(len(evi))] + [SEP]\n",
    "        seg_li += [cur_seg for _ in range(len(evi)+1)]\n",
    "        # cur_seg += 1\n",
    "    return output, seg_li\n",
    "\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self, input_df, max_len_claim, max_len_evi, max_evi, num_class):\n",
    "        self.df = input_df\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_len_claim = max_len_claim\n",
    "        self.max_len_evi = max_len_evi\n",
    "        self.max_evi = max_evi\n",
    "        self.num_class = num_class\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ## claim_text, claim_label, evidences\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        claim = self.df.loc[index, 'claim']\n",
    "        label = self.df.loc[index, 'label']\n",
    "        evidence = self.df.loc[index, 'evidence']\n",
    "\n",
    "        #Preprocessing the claim\n",
    "        claim = self.tokenizer.tokenize(claim)\n",
    "        evidence = [self.tokenizer.tokenize(i) for i in evidence]\n",
    "        claim = pad_sen(claim, self.max_len_claim)\n",
    "        evidence = [pad_sen(i, self.max_len_evi) for i in evidence]\n",
    "        input_token, seg_li = relation_sep(claim, evidence, self.max_evi)\n",
    "        attn_mask = [1 if token != PAD else 0 for token in input_token]\n",
    "        input_token = self.tokenizer.convert_tokens_to_ids(input_token)\n",
    "        input_id = torch.tensor(input_token) #Converting the list to a pytorch tensor\n",
    "        attn_mask = torch.tensor(attn_mask)\n",
    "        seg_li = torch.tensor(seg_li)\n",
    "        labels = np.zeros(self.num_class)\n",
    "        np.put(labels, label, 1)\n",
    "        label = torch.tensor(labels)\n",
    "        return input_id, attn_mask, seg_li, label\n",
    "\n",
    "def predict_val(net, dataloader, gpu):\n",
    "    net.eval()\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, seg_li, _ in dataloader:\n",
    "            seq, attn_masks, seg_li = seq.cuda(gpu), attn_masks.cuda(gpu), seg_li.cuda(gpu)\n",
    "            ## HERE\n",
    "            output = net(seq, attn_masks, seg_li)\n",
    "            # output = np.array(output.tolist())\n",
    "            for score in output:\n",
    "                if score[0].item() < 0.6 and score[1].item() < 0.6:\n",
    "                    pred.append(label_trans[NOT_ENOUGH_INFO])\n",
    "                    \n",
    "                elif np.absolute(score[0].item() - score[1].item()) < 0.05:\n",
    "                    pred.append(label_trans[DISPUTED])\n",
    "                else:\n",
    "                    pred.append(torch.argmax(score).item())\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_df = pd.read_pickle(VERI_DF_PATH)\n",
    "\n",
    "val_df = veri_df[[\"claim\", \"evidence\", \"evidence_id\", \"claim_id\"]]\n",
    "val_df[\"label\"] = 0\n",
    "\n",
    "# read data\n",
    "# val_df = veri_df[[\"claim\", \"evidence\", \"label\", \"evidence_id\", \"claim_id\"]]\n",
    "# val_df[\"label\"] = veri_df.apply(lambda x: label_trans[x[\"label\"]], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 2, 0, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 1, 0, 2, 0, 2, 2, 0, 3, 1, 1, 1, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 2, 3, 2, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 2, 0, 1, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 0, 1, 0, 3, 0, 0, 0, 2, 2, 1, 0, 0, 2, 2, 2, 1, 1, 1, 0, 2, 2, 2, 0, 1, 0, 2, 2, 1, 3, 2, 2, 1, 1, 0, 1, 2, 2, 0, 2, 3, 2, 2, 2, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# read data / model and set configuration\n",
    "MAX_EVI = 3\n",
    "num_class_val = 4\n",
    "val_mod = RelClassifier(num_class_val)\n",
    "val_mod.load_state_dict(torch.load(VAL_MODEL_PATH))\n",
    "val_mod.cuda(gpu) #Enable gpu support for the model\n",
    "\n",
    "val_ds = ValDataset(val_df[['claim', 'evidence', 'label']], PAD_CLAIM, PAD_EVIDENCE, MAX_EVI, num_class_val)\n",
    "val_dl = DataLoader(val_ds, batch_size = BATCH_SIZE, num_workers = NO_WORKER)\n",
    "\n",
    "\n",
    "gpu = 0 #gpu ID\n",
    "pred_label = predict_val(val_mod, val_dl, gpu)\n",
    "print(pred_label)\n",
    "val_df[\"pred_label\"] = pred_label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Bert evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(confusion_matrix(val_df.label.values.tolist(), pred_label))\n",
    "# print(pred_label)\n",
    "# print(val_df.label.values.tolist())\n",
    "# print(np.mean(np.array(val_df.label.values.tolist()) == np.array(pred_label)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   claim_text  \\\n",
      "claim                                                           \n",
      "claim-1001  ‘This study goes beyond statistical correlatio...   \n",
      "claim-1003  A recent study in Nature Geoscience, for insta...   \n",
      "claim-1009  ‘Arctic ice conditions have been tracking at r...   \n",
      "claim-1020  “The global reef crisis does not necessarily m...   \n",
      "claim-1028  A second coat of paint has much less of an eff...   \n",
      "...                                                       ...   \n",
      "claim-910   The cement, iron and steel, and petroleum refi...   \n",
      "claim-942   ‘We could be decades too fast, or decades too ...   \n",
      "claim-952   The Alaskan tundra is warming so quickly it ha...   \n",
      "claim-972   “Arctic land stores about twice as much carbon...   \n",
      "claim-979   “Warm weather worsened the most recent five-ye...   \n",
      "\n",
      "                claim_label                                          evidences  \n",
      "claim                                                                           \n",
      "claim-1001         SUPPORTS                 [evidence-895046, evidence-277435]  \n",
      "claim-1003         SUPPORTS  [evidence-829081, evidence-430670, evidence-18...  \n",
      "claim-1009  NOT_ENOUGH_INFO  [evidence-482300, evidence-649967, evidence-74...  \n",
      "claim-1020         SUPPORTS  [evidence-382341, evidence-754568, evidence-19...  \n",
      "claim-1028  NOT_ENOUGH_INFO  [evidence-18609, evidence-154614, evidence-644...  \n",
      "...                     ...                                                ...  \n",
      "claim-910   NOT_ENOUGH_INFO  [evidence-117114, evidence-197221, evidence-71...  \n",
      "claim-942   NOT_ENOUGH_INFO                                                 []  \n",
      "claim-952   NOT_ENOUGH_INFO  [evidence-705706, evidence-855683, evidence-28...  \n",
      "claim-972   NOT_ENOUGH_INFO  [evidence-22100, evidence-450316, evidence-620...  \n",
      "claim-979          SUPPORTS  [evidence-957389, evidence-434312, evidence-17...  \n",
      "\n",
      "[153 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "rev_label_trans = {0: SUPPORTS, 1: REFUTES, 2: NOT_ENOUGH_INFO, 3: DISPUTED}\n",
    "\n",
    "red_df = val_df[[\"claim\", \"pred_label\", \"evidence_id\" ,\"claim_id\"]].rename({\"pred_label\": \"label\"}, axis=1)\n",
    "red_df = red_df.set_index(\"claim_id\")\n",
    "known_claim = red_df.index.values\n",
    "drop_df = pd.DataFrame(columns=[\"claim\", \"claim_text\", \"claim_label\", \"evidences\"])\n",
    "for record in claim_df.values:\n",
    "    if record[0] in known_claim:\n",
    "        drop_df = drop_df.append({\"claim\": record[0], \"claim_text\": red_df.loc[record[0]][\"claim\"], \"claim_label\": rev_label_trans[red_df.loc[record[0]][\"label\"]], \"evidences\": red_df.loc[record[0]][\"evidence_id\"]}, ignore_index=True)\n",
    "    else:\n",
    "        drop_df = drop_df.append({\"claim\": record[0], \"claim_text\": record[1], \"claim_label\": NOT_ENOUGH_INFO, \"evidences\": []}, ignore_index=True)\n",
    "drop_df = drop_df.set_index(\"claim\")\n",
    "print(drop_df)\n",
    "drop_df.to_json(\"test-claims-predictions.json\", orient=\"index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“Warm weather worsened the most recent five-year drought, which included the driest four-year period on record in terms of statewide precipitation.'\n",
      " 'SUPPORTS'\n",
      " list(['evidence-957389', 'evidence-434312', 'evidence-178433'])]\n",
      "defaultdict(<class 'int'>, {'SUPPORTS': 48, 'NOT_ENOUGH_INFO': 79, 'REFUTES': 21, 'DISPUTED': 5})\n"
     ]
    }
   ],
   "source": [
    "count_dict  = defaultdict(int)\n",
    "for record in drop_df.values:\n",
    "    count_dict[record[1]] += 1\n",
    "    continue\n",
    "print(record)\n",
    "\n",
    "print(count_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
